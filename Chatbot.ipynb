{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYQp3At0FBI4DGMxGeeuGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anou26/Projects/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQZ4DB8pmbEY"
      },
      "source": [
        "This project aims to build a basic chat-bot in python using NLTK. It's a simple bot but a great way for one to understand NLP.\n",
        "\n",
        "**About NLP**\n",
        "Using NLP (Natural Language Processing), computers can analyse , understand and derive meaning from human language in an effective way. NLP acts as a great way for developers to progress into tasks pertaining to speech-recognition, sentiment analysis, automatic summarization, translation as well as named entity recognition.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9BxSX12nhuo"
      },
      "source": [
        "#import libraries\n",
        "import io\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRGTfpKGxVym"
      },
      "source": [
        "In the above code snippet, we imported libraries from python such as NumPy and sklearn for supporting classification algorithms in our program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAJSLCxBx1lH"
      },
      "source": [
        "Now, let's install NLTK. NLTK or Natural Language Toolkit is a platform used in python  programming to work with human language. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAm-PuDn1IUw",
        "outputId": "b15b8e13-8c80-4577-b9e9-680b4da08725"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4pVl-xP1L6n",
        "outputId": "91eb4e6b-6b70-4d70-bec6-535681107889"
      },
      "source": [
        "#importing nltk packages\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular', quiet=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTUlR2Zx1m9I"
      },
      "source": [
        "In the above code snippet, we imported nltk packages and obtained a boolean value True.\n",
        "Now, let's read in the corpus. By this, we mean use a page (here: wikipedia) for implementing text-processing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLIxVCqV4lV4",
        "outputId": "eef3bc8c-5cd9-478e-c702-66657b337a62"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6oAq6ty3gIX"
      },
      "source": [
        "f=open('/content/drive/My Drive/chatbot.txt','r',errors = 'ignore')\n",
        "raw=f.read()\n",
        "raw = raw.lower()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5fES6Ge5ery"
      },
      "source": [
        "In the above code snippets, we placed our corpus text into a textfile and mounted it. Here, we're calling it chatbot.txt\n",
        "The f.read() would help us read the file.\n",
        "The raw.lower() would help us convert the text into lowercase. This is ideal as for any NLP project, we need to pre-process it to make it effective for working. \n",
        "Text-processing basically involves, \n",
        "*   Converting the whole text to either upper-case or lower-case\n",
        "*   Tokenization\n",
        "\n",
        "The NLTK package contains a pre-trained Punkt tokenizer for English.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW_YX3EB6mMI"
      },
      "source": [
        "#Tokenization\n",
        "sent_tokens = nltk.sent_tokenize(raw)\n",
        "word_tokens = nltk.word_tokenize(raw)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTIaruW06xnT"
      },
      "source": [
        "In the above code snippets,\n",
        "*   sent_tokens converts text into a list of sentences\n",
        "*   word_tokens converts text into a list of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPgoXwI069Pf"
      },
      "source": [
        "#Pre-processing\n",
        "lem = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lem.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrvImEZQ7chA"
      },
      "source": [
        "In the above code snippet, we define a function called LemTokens where we input tokens and return normalised tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpVUNGw_7uN8"
      },
      "source": [
        "#Keyword Matching\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sssmkuBx8Giu"
      },
      "source": [
        "We can see above that for every input greeting, we have a standardised response. This forms the base for initiating conversation with our chatbot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc42RbYT8Qll"
      },
      "source": [
        "# Cosine Similarity\n",
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I'm sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHQzwJ4f9IIX"
      },
      "source": [
        "Now, let's write code for our conversation initiation and ending. This is the fun part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAWo2JLi8blz",
        "outputId": "0d070a51-c834-4735-dadf-77bb2702f2cf"
      },
      "source": [
        "flag=True\n",
        "print(\"BOBtheRobo: My name is Bob. I'm here if you wanna chat. If you don't, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"BOBtheRobo: You are welcome!\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"BOBtheRobo: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"BOBtheRobo: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! See ya later\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOBtheRobo: My name is Bob. I'm here if you wanna chat. If you don't, type Bye!\n",
            "Hey\n",
            "BOBtheRobo: hi there\n",
            "Hello\n",
            "BOBtheRobo: hi\n",
            "Bye\n",
            "ROBO: Bye! See ya later\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}